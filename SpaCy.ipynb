{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INTRODUCTION TO NLP IN PYTHON WITH SPACY\n",
    "\n",
    "spaCy is a relatively new package for ‚ÄúIndustrial strength NLP in Python‚Äù developed by Matt Honnibal at Explosion AI. It is designed with the applied data scientist in mind, meaning it does not weigh the user down with decisions over what esoteric algorithms to use for common tasks and it‚Äôs fast. Incredibly fast (it‚Äôs implemented in Cython). If you are familiar with the Python data science stack, spaCy is your numpy for NLP ‚Äì it‚Äôs reasonably low-level, but very intuitive and performant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installation**\n",
    "\n",
    "Thanks to our great community, we've finally re-added conda support. You can now install spaCy via conda-forge:\n",
    "\n",
    "conda install -c conda-forge spacy\n",
    "\n",
    "\n",
    "\n",
    "Using pip, spaCy releases are currently only available as source packages.\n",
    "\n",
    "\n",
    "pip install -U spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installation you need to download a language model. For more info and available models, see the docs on models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load spaCy‚Äôs pipeline, which by convention is stored in a variable named nlp. declaring this variable will take a couple of seconds as spaCy loads its models and data to it up-front to save time later. In effect, this gets some heavy lifting out of the way early, so that the cost is not incurred upon each application of the nlp parser to your data. Note that here I am using the English language model, but there is also a fully featured German model, with tokenisation (discussed below) implemented across several languages.\n",
    "\n",
    "We invoke nlp on the sample text to create a Doc object. The Doc object is now a vessel for NLP tasks on the text itself, slices of the text (Span objects) and elements (Token objects) of the text. It is worth noting that Token and Span objects actually hold no data. Instead they contain pointers to data contained in the Doc object and are evaluated lazily (i.e. upon request). Much of spaCy‚Äôs core functionality is accessed through the methods on Doc (n=33), Span (n=29) and Token (n=78) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(\"The big grey dog ate all of the chocolate,but fortunately he wasn't sick!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "\n",
    "Tokenisation is a foundational step in many NLP tasks. Tokenising text is the process of splitting a piece of text into words, symbols, punctuation, spaces and other elements, thereby creating ‚Äútokens‚Äù. A naive way to do this is to simply split the string on white space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'big',\n",
       " 'grey',\n",
       " 'dog',\n",
       " 'ate',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'chocolate,but',\n",
       " 'fortunately',\n",
       " 'he',\n",
       " \"wasn't\",\n",
       " 'sick!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linguistic annotations\n",
    "spaCy provides a variety of linguistic annotations to give you insights into a text's grammatical structure. This includes the word types, like the parts of speech, and how the words are related to each other. For example, if you're analysing text, it makes a huge difference whether a noun is the subject of a sentence, or the object ‚Äì or whether \"google\" is used as a verb, or refers to the website or company in a specific context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is VERB aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off ‚Äì whereas \"U.K.\" should remain one token. Each Doc consists of individual tokens, and we can simply iterate over them:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-of-speech tags and dependencies\n",
    "After tokenization, spaCy can parse and tag a given Doc. This is where the statistical model comes in, which enables spaCy to make a prediction of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalise across the language ‚Äì for example, a word following \"the\" in English is most likely a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entities \n",
    "A named entity is a \"real-world object\" that's assigned a name ‚Äì for example, a person, a country, a product or a book title. spaCy can recognise various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn't always work perfectly and might need some tuning later, depending on your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Word vectors and similarity\n",
    "\n",
    "spaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that's similar to what they're currently looking at, or label a support ticket as a duplicate if it's very similar to an already existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.801686\n",
      "dog banana 0.243276\n",
      "cat dog 0.801686\n",
      "cat cat 1.0\n",
      "cat banana 0.281544\n",
      "banana dog 0.243276\n",
      "banana cat 0.281544\n",
      "banana banana 1.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#nlp = spacy.load('en_core_web_md')  # make sure to use larger model!\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokens = nlp(u'dog cat banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.03367 False\n",
      "cat True 6.68082 False\n",
      "banana True 6.70001 False\n",
      "afskfsd False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#nlp = spacy.load('en_core_web_md')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokens = nlp(u'dog cat banana afskfsd')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab, hashes and lexemes\n",
    "\n",
    "Whenever possible, spaCy tries to store data in a vocabulary, the Vocab , that will be shared by multiple documents. To save memory, spaCy also encodes all strings to hash values ‚Äì in this case for example, \"coffee\" has the hash 3197928453018144401. Entity labels like \"ORG\" and part-of-speech tags like \"VERB\" are also encoded. Internally, spaCy only \"speaks\" in hash values.\n",
    "\n",
    "Token: A word, punctuation mark etc. in context, including its attributes, tags and dependencies.\n",
    "Lexeme: A \"word type\" with no context. Includes the word shape and flags, e.g. if it's lowercase, a digit or punctuation.\n",
    "Doc: A processed container of tokens in context.\n",
    "Vocab: The collection of lexemes.\n",
    "StringStore: The dictionary mapping hash values to strings, for example 3197928453018144401 ‚Üí \"coffee\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n",
      "coffee\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I love coffee')\n",
    "print(doc.vocab.strings[u'coffee'])  # 3197928453018144401\n",
    "print(doc.vocab.strings[3197928453018144401])  # 'coffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "love 3702023516439754181 xxxx l ove True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I love coffee')\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "          lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n",
      "coffee\n",
      "coffee\n",
      "coffee\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I love coffee') # original Doc\n",
    "print(doc.vocab.strings[u'coffee'])  # 3197928453018144401\n",
    "print(doc.vocab.strings[3197928453018144401])  # 'coffee' üëç\n",
    "\n",
    "empty_doc = Doc(Vocab())  # new Doc with empty Vocab\n",
    "# empty_doc.vocab.strings[3197928453018144401] will raise an error :(\n",
    "\n",
    "empty_doc.vocab.strings.add(u'coffee')  # add \"coffee\" and generate hash\n",
    "print(empty_doc.vocab.strings[3197928453018144401])  # 'coffee' üëç\n",
    "\n",
    "new_doc = Doc(doc.vocab)  # create new doc with first doc's vocab\n",
    "print(new_doc.vocab.strings[3197928453018144401])  # 'coffee' üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Serialization\n",
    "If you've been modifying the pipeline, vocabulary, vectors and entities, or made updates to the model, you'll eventually want to save your progress ‚Äì for example, everything that's in your nlp object. This means you'll have to translate its contents and structure into a format that can be saved, like a file or a byte string. This process is called serialization. spaCy comes with built-in serialization methods and supports the Pickle protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text = open('customer_feedback_627.txt', 'r').read() # open a document\n",
    "#doc = nlp(text) # process it\n",
    "#doc.to_disk('/customer_feedback_627.bin') # save the processed Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need it again later, you can load it back into an empty Doc with an empty Vocab by calling from_disk() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from spacy.tokens import Doc # to create empty Doc\n",
    "#from spacy.vocab import Vocab # to create empty Vocab\n",
    "\n",
    "#doc = Doc(Vocab()).from_disk('/customer_feedback_627.bin') # load processed Doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install models and process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#python -m spacy download en_core_web_sm\n",
    "#python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Here', 'are', 'two', 'sentences', '.']\n",
      "['Ich', 'bin', 'ein', 'Berliner', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Hello, world. Here are two sentences.')\n",
    "print([t.text for t in doc])\n",
    "\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "doc_de = nlp_de(u'Ich bin ein Berliner.')\n",
    "print([t.text for t in doc_de])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get tokens, noun chunks & sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peach\n",
      "emoji\n",
      "üçë\n",
      "outranking eggplant\n",
      "Peach emoji\n",
      "Peach is the superior emoji.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Peach emoji is where it has always been. Peach is the superior \"\n",
    "          u\"emoji. It's outranking eggplant üçë \")\n",
    "print(doc[0].text)          # Peach\n",
    "print(doc[1].text)          # emoji\n",
    "print(doc[-1].text)         # üçë\n",
    "print(doc[17:19].text)      # outranking eggplant\n",
    "\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "print(noun_chunks[0].text)  # Peach emoji\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "assert len(sentences) == 3\n",
    "print(sentences[1].text)    # 'Peach is the superior emoji.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get part-of-speech tags and flags \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-grained POS tag PROPN 95\n",
      "Coarse-grained POS tag NNP 15794550382381185553\n",
      "Word shape Xxxxx 16072095006890171862\n",
      "Alphanumeric characters? True\n",
      "Punctuation mark? False\n",
      "Digit? False\n",
      "Like a number? True\n",
      "Like an email address? False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "apple = doc[0]\n",
    "print('Fine-grained POS tag', apple.pos_, apple.pos)\n",
    "print('Coarse-grained POS tag', apple.tag_, apple.tag)\n",
    "print('Word shape', apple.shape_, apple.shape)\n",
    "print('Alphanumeric characters?', apple.is_alpha)\n",
    "print('Punctuation mark?', apple.is_punct)\n",
    "\n",
    "billion = doc[10]\n",
    "print('Digit?', billion.is_digit)\n",
    "print('Like a number?', billion.like_num)\n",
    "print('Like an email address?', billion.like_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Use hash values for any string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401 coffee\n",
      "3197928453018144401 3197928453018144401\n",
      "coffee coffee\n",
      "3073001599257881079 beer\n",
      "17758882941175878347 ü¶Ñ \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I love coffee')\n",
    "\n",
    "coffee_hash = nlp.vocab.strings[u'coffee']  # 3197928453018144401\n",
    "coffee_text = nlp.vocab.strings[coffee_hash]  # 'coffee'\n",
    "print(coffee_hash, coffee_text)\n",
    "print(doc[2].orth, coffee_hash)  # 3197928453018144401\n",
    "print(doc[2].text, coffee_text)  # 'coffee'\n",
    "\n",
    "beer_hash = doc.vocab.strings.add(u'beer')  # 3073001599257881079\n",
    "beer_text = doc.vocab.strings[beer_hash]  # 'beer'\n",
    "print(beer_hash, beer_text)\n",
    "\n",
    "unicorn_hash = doc.vocab.strings.add(u'ü¶Ñ ')  # 18234233413267120783\n",
    "unicorn_text = doc.vocab.strings[unicorn_hash]  # 'ü¶Ñ '\n",
    "print(unicorn_hash, unicorn_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recognise and update named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco 0 13 GPE\n",
      "FB 0 2 ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'San Francisco considers banning sidewalk delivery robots')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "from spacy.tokens import Span\n",
    "doc = nlp(u'FB is hiring a new VP of global policy')\n",
    "doc.ents = [Span(doc, 0, 1, label=doc.vocab.strings[u'ORG'])]\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and update neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "train_data = [(\"Uber blew through $1 million\", {'entities': [(0, 4, 'ORG')]})]\n",
    "\n",
    "with nlp.disable_pipes(*[pipe for pipe in nlp.pipe_names if pipe != 'ner']):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(10):\n",
    "        random.shuffle(train_data)\n",
    "        for text, annotations in train_data:\n",
    "            nlp.update([text], [annotations], sgd=optimizer)\n",
    "nlp.to_disk('/Users/nanaakwasiabayieboateng/PythonNLTK/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a dependency parse and named entities in your browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'dep' visualizer\n",
      "\n",
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n",
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'ent' visualizer\n",
      "\n",
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc_dep = nlp(u'This is a sentence.')\n",
    "displacy.serve(doc_dep, style='dep')\n",
    "\n",
    "doc_ent = nlp(u'When Sebastian Thrun started working on self-driving cars at Google '\n",
    "              u'in 2007, few people outside of the company took him seriously.')\n",
    "displacy.serve(doc_ent, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get word vectors and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple <-> banana 0.583184\n",
      "pasta <-> hippo 0.0793491\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "#nlp = spacy.load('en_core_web_md')  # doesn't work\n",
    "#nlp = spacy.load('en_core_web_sm') #works to\n",
    "nlp = spacy.load('en_core_web_lg') #gives best results\n",
    "\n",
    "#nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(u\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
    "\n",
    "apple = doc[0]\n",
    "banana = doc[2]\n",
    "pasta = doc[6]\n",
    "hippo = doc[8]\n",
    "\n",
    "print('apple <-> banana', apple.similarity(banana))\n",
    "print('pasta <-> hippo', pasta.similarity(hippo))\n",
    "print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week DATE\n",
      "my fries were super gross such disgusting fries 0.713970251872\n"
     ]
    }
   ],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process whole documents\n",
    "text = (u\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        u\"Google in 2007, few people outside of the company took him \"\n",
    "        u\"seriously. ‚ÄúI can tell you very senior CEOs of major American \"\n",
    "        u\"car companies would shake my hand and turn away because I wasn‚Äôt \"\n",
    "        u\"worth talking to,‚Äù said Thrun, now the co-founder and CEO of \"\n",
    "        u\"online higher education startup Udacity, in an interview with \"\n",
    "        u\"Recode earlier this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "\n",
    "# Determine semantic similarities\n",
    "doc1 = nlp(u\"my fries were super gross\")\n",
    "doc2 = nlp(u\"such disgusting fries\")\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(doc1.text, doc2.text, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple and efficient serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "#customer_feedback = open('customer_feedback_627.txt').read()\n",
    "#doc = nlp(customer_feedback)\n",
    "#doc.to_disk('/Users/nanaakwasiabayieboateng/PythonNLTK/customer_feedback_627.bin')\n",
    "\n",
    "#new_doc = Doc(Vocab()).from_disk('/Users/nanaakwasiabayieboateng/PythonNLTK/customer_feedback_627.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match text with token rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleIO Google I/O\n",
      "HAPPY üòÄüòÄ\n",
      "Sentiment 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def set_sentiment(matcher, doc, i, matches):\n",
    "    doc.sentiment += 0.1\n",
    "\n",
    "pattern1 = [{'ORTH': 'Google'}, {'ORTH': 'I'}, {'ORTH': '/'}, {'ORTH': 'O'}]\n",
    "pattern2 = [[{'ORTH': emoji, 'OP': '+'}] for emoji in ['üòÄ', 'üòÇ', 'ü§£', 'üòç']]\n",
    "matcher.add('GoogleIO', None, pattern1) # match \"Google I/O\" or \"Google i/o\"\n",
    "matcher.add('HAPPY', set_sentiment, *pattern2) # match one or more happy emoji\n",
    "\n",
    "doc = nlp(u\"A text about Google I/O üòÄüòÄ\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)\n",
    "print('Sentiment', doc.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-threaded generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [u'One document.', u'...', u'Lots of documents']\n",
    "# .pipe streams input, and produces streaming output\n",
    "#iter_texts = (texts[i % 3] for i in xrange(100000000))\n",
    "#for i, doc in enumerate(nlp.pipe(iter_texts, batch_size=50, n_threads=4)):\n",
    "#    assert doc.is_parsed\n",
    "#    if i == 100:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get syntactic dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advmod', 'advcl', 'compound', 'nsubj', 'advcl', 'nsubj', 'advcl', 'advcl', 'xcomp', 'advcl', 'prep', 'xcomp', 'advcl', 'npadvmod', 'amod', 'pobj', 'prep', 'xcomp', 'advcl', 'punct', 'amod', 'pobj', 'prep', 'xcomp', 'advcl', 'amod', 'pobj', 'prep', 'xcomp', 'advcl', 'pobj', 'prep', 'xcomp', 'advcl', 'prep', 'xcomp', 'advcl', 'pobj', 'prep', 'xcomp', 'advcl', 'prep', 'xcomp', 'advcl', 'pobj', 'prep', 'xcomp', 'advcl', 'punct', 'amod', 'nsubj', 'nsubj', 'prep', 'nsubj', 'prep', 'prep', 'nsubj', 'det', 'pobj', 'prep', 'prep', 'nsubj', 'pobj', 'prep', 'prep', 'nsubj', 'dobj', 'advmod', 'punct']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"When Sebastian Thrun started working on self-driving cars at Google \"\n",
    "          u\"in 2007, few people outside of the company took him seriously.\")\n",
    "\n",
    "dep_labels = []\n",
    "for token in doc:\n",
    "    while token.head != token:\n",
    "        dep_labels.append(token.dep_)\n",
    "        token = token.head\n",
    "print(dep_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 8104846059040039827 False\n",
      "out 1696981056005371314 False\n",
      "https://spacy.io 17142293684782158888 True\n",
      "(3, 2)\n",
      "3 2\n",
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.attrs import ORTH, LIKE_URL\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Check out https://spacy.io\")\n",
    "for token in doc:\n",
    "    print(token.text, token.orth, token.like_url)\n",
    "\n",
    "attr_ids = [ORTH, LIKE_URL]\n",
    "doc_array = doc.to_array(attr_ids)\n",
    "print(doc_array.shape)\n",
    "print(len(doc), len(attr_ids))\n",
    "\n",
    "assert doc[0].orth == doc_array[0, 0]\n",
    "assert doc[1].orth == doc_array[1, 0]\n",
    "assert doc[0].like_url == doc_array[0, 1]\n",
    "\n",
    "assert list(doc_array[:, 1]) == [t.like_url for t in doc]\n",
    "print(list(doc_array[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Calculate inline markup on original string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://spacy.io'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def put_spans_around_tokens(doc):\n",
    "    \"\"\"Here, we're building a custom \"syntax highlighter\" for\n",
    "    part-of-speech tags and dependencies. We put each token in a\n",
    "    span element, with the appropriate classes computed. All whitespace is\n",
    "    preserved, outside of the spans. (Of course, HTML will only display\n",
    "    multiple whitespace if enabled ‚Äì but the point is, no information is lost\n",
    "    and you can calculate what you need, e.g. <br />, <p> etc.)\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    html = '<span class=\"{classes}\">{word}</span>{space}'\n",
    "    for token in doc:\n",
    "        if token.is_space:\n",
    "            output.append(token.text)\n",
    "        else:\n",
    "            classes = 'pos-{} dep-{}'.format(token.pos_, token.dep_)\n",
    "            output.append(html.format(classes=classes, word=token.text, space=token.whitespace_))\n",
    "    string = ''.join(output)\n",
    "    string = string.replace('\\n', '')\n",
    "    string = string.replace('\\t', '    ')\n",
    "    return '<pre>{}</pre>.format(string)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"This is a test.\\n\\nHello   world.\")\n",
    "html = put_spans_around_tokens(doc)\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
